#!/bin/bash
#
# 4chget - a simple script for downloading 4chan images
#            by Greg Naughton, (c) 2011
#


Color_Off='\e[0m'       # reset color
BIRed='\e[1;91m'        # Red
BIGreen='\e[1;92m'      # Green
BIYellow='\e[1;93m'     # Yellow
BIBlue='\e[1;94m'       # Blue
BIPurple='\e[1;95m'     # Purple
BICyan='\e[1;96m'       # Cyan
BIWhite='\e[1;97m'      # White


if [ ! $1 ]; then
    echo usage: $(basename $0)" [-A] <4chan url> <dir to save img dump>"
    echo "  1st arg: 4chan page URL"
    echo "  2nd arg (optional): directory to dump images to"
    echo "   * if no dir provided, images will be dumped into current path"
    echo "   * if -A option is used, the entire page will be archived"
    exit
fi

ARG1=$1; ARG2=$2
if [ $1 == "-A" ]; then ARCHIVE=1; ARG1=$2; ARG2=$3; fi


OUTDIR=`echo $ARG1 | sed -r -e 's/(.*)\///g'` # moot helpfully gives us thread names in the url now ^_^

if [ $ARG2 ]; then
    OUTDIR=$ARG2
fi

if [ ! -d $OUTDIR ]; then
	echo directory: \"$OUTDIR\" doesn\'t exist. making it.
	mkdir $OUTDIR
fi
cd $OUTDIR

echo saving images to \"$OUTDIR\"


COUNT=0
SKIP=0

# default to wget if it's available
if which wget >/dev/null; then HAVE_WGET=true; fi

# 404 - proceed no further to prevent overwrites 
PAGE=`curl -b -j -c ~/.4chan_cookies.txt $ARG1` 
if echo "$PAGE" | grep -q "404 Not Found"; then
    echo -e "${BIRed}Page 404'd, no longer exists${Color_Off}"
    exit;
fi


URLS_BIG=`echo $PAGE | tr ' ' '\n' | grep -E \/\/i.4cdn.org\/[a-z]+\/[0-9]*\.[jpg\|png\|gif\|webm] | sed -e 's/href="//g' -e 's/"//g' | uniq`
if [ $(echo $URLS_BIG | wc -l | awk '{print $1}') -le 2 ]; then
	URLS_BIG=`echo $URLS_BIG | tr ' ' '\n'`
fi
I=0
for line in ${URLS_BIG}; do
  URLS[$I]=$line
  let I="$I+1"
done


#
# ARCHIVE option downloads html and thumbnails as well
#
if [ $ARCHIVE ]; then
    echo Archiving Page with Thumbnails
    PAGE_NUM=`echo $ARG1 | sed -E 's_http:\/\/boards.4chan.org\/[a-z]+\/res\/__'`
    TDIR=thumbs_${PAGE_NUM}
    THUMB_URLS=`echo $PAGE | tr ' ' '\n' | grep -E thumbs.4chan.org\/[a-z]+\/thumb\/[0-9]*s\.[jpg][pni][gf] | sed -e 's/src=//g' | uniq`
    PAGE=`echo $PAGE | sed -E -e 's_\/\/images.4chan.org\/[a-z]+\/src\/__g' -e "s^\/\/[0-9]+.thumbs.4chan.org\/[a-z]+\/thumb^${TDIR}^g"`

    OUT=`if [ $ARG2 ]; then echo $(basename $ARG2)--$PAGE_NUM; else echo $PAGE_NUM; fi`
    echo $PAGE > $OUT.html

    #get thumbs, to a directory named 'thumbs_page#'
    if [ ! -e $TDIR ]; then mkdir ${TDIR}; fi

    for thurl in $THUMB_URLS; do 
        echo "$thurl" | grep -q "http" || thurl='http:'$thurl
        thurl=`echo "$thurl" | sed -e 's/\"//g'`
        tname=`echo $thurl | sed -E 's_http:\/\/[0-9]+.thumbs.4chan.org\/[a-z]+\/thumb\/__'`
        if [ ! -e ${TDIR}/${tname} ]; then
            curl $thurl -o ${TDIR}/${tname}
        fi
    done
fi

function get_line() {
    local line=$1
    if $HAVE_WGET; then
        wget -qc $line &
    else
        curl -C - $line -o $pic
    fi
}

I=0
MSG=false
RESUMED=false
TO_GET=0
while [ $I -lt ${#URLS[*]} ]; 
do

    line=${URLS[$I]}

    let NPONE="$I+1"

    I=$NPONE

    if [ $NPONE -lt ${#URLS[*]} ]; then
        next_line=${URLS[$NPONE]}
    else
        next_line=''
    fi

    echo "$line" | grep -q "http" || line='http:'$line
    echo "$next_line" | grep -q "http" || next_line='http:'$next_line

    pic=`echo $line | sed -r 's^http:\/\/i.4cdn.org\/[a-z]+\/^^g'`
    next_pic=`echo $next_line | sed -r 's^http:\/\/i.4cdn.org\/[a-z]+\/^^g'`


    # skip
    if [ -e $pic ] && [ -e $next_pic ]; then
        let SKIP="${SKIP}+1"
        echo skipping "${pic} (#${SKIP})", already have it 

    # reget last one, usually these might be interrupted
    elif [ -e $pic ] && [ ! -e $next_pic ]; then
        if ! $RESUMED ; then 
            echo "regetting last pic, in case it wasn't complete"
            RESUMED=true;
        fi

        get_line $line

    # any missing in the middle
    elif [ -e $next_pic ] && [ ! -e $pic ] ; then
        let COUNT="${COUNT}+1"
        echo -e "${BIYellow}$COUNT${Color_Off}"
        get_line $line

    # continue getting missing at the end
    elif [ ! -e $pic ] ; then

        if ! $MSG; then 
          let TO_GET="${#URLS[*]}-$I"; 
          if [ $TO_GET -gt 0 ]; then
            echo; echo -e " >>> ${BIYellow}$TO_GET${Color_Off} more images to get";echo;
          fi
          MSG=true; 
        fi

        let COUNT="${COUNT}+1"
        echo -e "${BIYellow}$COUNT/$TO_GET${Color_Off}"
        get_line $line

    else
        let SKIP="${SKIP}+1"
        echo skipping "${pic} (#${SKIP})", already have it 
    fi
done 

if [ $SKIP -gt 0 ]; then
    echo -n -e "skipped getting ${BIPurple}$SKIP${Color_Off} files, "
fi

if [ $COUNT -gt 0 ]; then
    echo -e "got ${BICyan}$COUNT${Color_Off} new images."
else
    echo -e "there were ${BIRed}NO${Color_Off} new images."
fi
