#!/bin/bash
#
# 4chget - a simple script for downloading 4chan images
#            by Greg Naughton, (c) 2011
#


Color_Off='\e[0m'       # reset color
BIRed='\e[1;91m'        # Red
BIGreen='\e[1;92m'      # Green
BIYellow='\e[1;93m'     # Yellow
BIBlue='\e[1;94m'       # Blue
BIPurple='\e[1;95m'     # Purple
BICyan='\e[1;96m'       # Cyan
BIWhite='\e[1;97m'      # White


if [ ! $1 ] || [ "$1" == "--help" ]; then
    echo usage: $(basename $0)" [-A] <4chan url> <dir to save img dump>"
    echo "  1st arg: 4chan page URL"
    echo "  2nd arg (optional): directory to dump images to"
    echo "   * if no dir provided, images will be dumped into current path"
    echo "   * if -A option is used, the entire page will be archived"
    echo "   * if -v option is used, verbose debug information is printed"
    exit
fi

ARG1=$1; ARG2=$2
if [ $1 == "-A" ]; then ARCHIVE=1; ARG1=$2; ARG2=$3;
elif [ $1 == "-v" ]; then VERBOSE=1; ARG1=$2; ARG2=$3;
fi

OUTDIR=`echo $ARG1 | sed -r -e 's/(.*)\///g'` # moot helpfully gives us thread names in the url now ^_^

if [ $ARG2 ]; then
    OUTDIR=$ARG2
fi

if [ ! -d $OUTDIR ]; then
	echo directory: \"$OUTDIR\" doesn\'t exist. making it.
	mkdir $OUTDIR
fi
cd $OUTDIR

echo saving images to \"$OUTDIR\"


COUNT=0
SKIP=0

# default to wget if it's available
if which wget >/dev/null; then HAVE_WGET=true; fi

# 404 - proceed no further to prevent overwrites
PAGE=`curl -b -j -c ~/.4chan_cookies.txt $ARG1 2>/dev/null`
if echo "$PAGE" | grep -q "404 Not Found"; then
    echo -e "${BIRed}Page 404'd, no longer exists${Color_Off}"
    exit 0
elif [ "k${PAGE}" == "k" ]; then
    echo -e "${BIRed}Connection timed out or no data returned.${Color_Off}"
    exit 0
fi

#was
IMAGE_HOST1='i.4cdn.org'
#is
IMAGE_HOST2='is2.4chan.org'

URLS_BIG1=`echo $PAGE | tr ' ' '\n' | grep -E \/\/${IMAGE_HOST1}\/[a-z]+\/[0-9]*\.[jpg\|png\|gif\|webm] | sed -e 's/href="//g' -e 's/"//g' | sed -e 's/https://g' -e 's/http://g' | uniq`
URLS_BIG2=`echo $PAGE | tr ' ' '\n' | grep -E \/\/${IMAGE_HOST2}\/[a-z]+\/[0-9]*\.[jpg\|png\|gif\|webm] | sed -e 's/href="//g' -e 's/"//g' | sed -e 's/https://g' -e 's/http://g' | uniq`

if [ $(echo $URLS_BIG1 | wc -l | awk '{print $1}') -le 2 ]; then
	URLS_BIG1=`echo $URLS_BIG1 | tr ' ' '\n'`
fi
if [ $(echo $URLS_BIG2 | wc -l | awk '{print $1}') -le 2 ]; then
	URLS_BIG2=`echo $URLS_BIG2 | tr ' ' '\n'`
fi
I=0
for line in ${URLS_BIG1}; do
  URLS[$I]=$line
  let I="$I+1"
done
for line in ${URLS_BIG2}; do
  URLS[$I]=$line
  let I="$I+1"
done

#
# BROKEN!... who cares
#
# ARCHIVE option downloads html and thumbnails as well
#
if [ $ARCHIVE ]; then
    echo Archiving option is broken
    exit 1

    echo Archiving Page with Thumbnails
    PAGE_NUM=`echo $ARG1 | sed -E 's_http:\/\/boards.4chan.org\/[a-z]+\/res\/__'`
    TDIR=thumbs_${PAGE_NUM}
    THUMB_URLS=`echo $PAGE | tr ' ' '\n' | grep -E thumbs.4chan.org\/[a-z]+\/thumb\/[0-9]*s\.[jpg][pni][gf] | sed -e 's/src=//g' | uniq`
    PAGE=`echo $PAGE | sed -E -e 's_\/\/images.4chan.org\/[a-z]+\/src\/__g' -e "s^\/\/[0-9]+.thumbs.4chan.org\/[a-z]+\/thumb^${TDIR}^g"`

    OUT=`if [ $ARG2 ]; then echo $(basename $ARG2)--$PAGE_NUM; else echo $PAGE_NUM; fi`
    echo $PAGE > $OUT.html

    #get thumbs, to a directory named 'thumbs_page#'
    if [ ! -e $TDIR ]; then mkdir ${TDIR}; fi

    for thurl in $THUMB_URLS; do
        echo "$thurl" | grep -q "http" || thurl='http:'$thurl
        thurl=`echo "$thurl" | sed -e 's/\"//g'`
        tname=`echo $thurl | sed -E 's_http:\/\/[0-9]+.thumbs.4chan.org\/[a-z]+\/thumb\/__'`
        if [ ! -e ${TDIR}/${tname} ]; then
            curl $thurl -o ${TDIR}/${tname}
        fi
    done
fi

function get_line() {
    local line=$1
    if $HAVE_WGET; then
        wget -qc $line
    else
        curl -C - $line -o $pic
    fi
}

I=0
MSG=false
RESUMED=false
TO_GET=0
while [ $I -lt ${#URLS[*]} ];
do

    line=${URLS[$I]}

    let NPONE="$I+1"

    if [ $NPONE -lt ${#URLS[*]} ]; then
        next_line=${URLS[$NPONE]}
    else
        next_line=''
    fi

    echo "$line" | grep -q "http" || line='http:'$line
    echo "$next_line" | grep -q "http" || next_line='http:'$next_line

    #pic=`echo $line | sed -r 's^http:\/\/'${IMAGE_HOST}'\/[a-z]+\/^^g'`
    #next_pic=`echo $next_line | sed -r 's^http:\/\/'${IMAGE_HOST}'\/[a-z]+\/^^g'`

    if [[ $line =~ .*"$IMAGE_HOST1".* ]]; then
        pic=`echo $line | sed -r 's^\/\/'${IMAGE_HOST1}'\/[a-z]+\/^^g'`
    else
        pic=`echo $line | sed -r 's^\/\/'${IMAGE_HOST2}'\/[a-z]+\/^^g'`
    fi

    if [[ $next_line =~ .*"$IMAGE_HOST1".* ]]; then
        next_pic=`echo $next_line | sed -r 's^\/\/'${IMAGE_HOST1}'\/[a-z]+\/^^g'`
    else
        next_pic=`echo $next_line | sed -r 's^\/\/'${IMAGE_HOST2}'\/[a-z]+\/^^g'`
    fi

    pic=`echo $pic | sed -e 's/https://g' -e 's/http://g'`
    next_pic=`echo $pic | sed -e 's/https://g' -e 's/http://g'`

    if [ "$VERBOSE" ]; then
        echo
        echo "------------------------"
        echo "count $COUNT"
        echo "NPONE $NPONE"
        echo "TO_GET $TO_GET"
        echo "pic $pic"
        echo "next_pic $next_pic"
        echo "#URLS ${#URLS[*]}"
        echo
    fi

    # skip
    if [ -e $pic ] && [ -e $next_pic ]; then
        let SKIP="${SKIP}+1"
        echo skipping "${pic} (#${SKIP})", already have it

    # reget last one, usually these might be interrupted
    elif [ -e $pic ] && [ ! -e $next_pic ] && [ ${NPONE} -ne ${#URLS[*]} ] ; then
        if ! $RESUMED ; then
            let SKIP="${SKIP}+1"
            echo "regetting $pic (#$SKIP), in case it wasn't complete"
            RESUMED=true;
        fi

        get_line $line

    # any missing in the middle
    elif [ -e $next_pic ] && [ ! -e $pic ] ; then
        let COUNT="${COUNT}+1"
        echo -e "${BIYellow}$COUNT${Color_Off}"
        get_line $line

    # continue getting missing at the end
    elif [ ! -e $pic ] ; then

        if ! $MSG; then
          let TO_GET="${#URLS[*]}-$I";
          if [ $TO_GET -gt 0 ]; then
            echo; echo -e " >>> ${BIYellow}$TO_GET${Color_Off} more images to get";echo;
          fi
          MSG=true;
        fi

        let COUNT="${COUNT}+1"
        echo -e "get ${BIYellow}$COUNT/$TO_GET${Color_Off} $pic (#$COUNT)"
        get_line $line

    else
        let SKIP="${SKIP}+1"
        echo skipping "${pic} (#${SKIP})", already have it
    fi

    I=$NPONE
done

if [ $SKIP -gt 0 ]; then
    echo -n -e "${BIWhite}${#URLS[*]}${Color_Off} total. Skipped getting ${BIPurple}$SKIP${Color_Off}. "
fi

if [ $COUNT -gt 0 ]; then
    echo -e "Got ${BICyan}$COUNT${Color_Off} new images."
else
    echo -e "There were ${BIRed}NO${Color_Off} new images."
fi

exit 1
